{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "96bbf2bd-88bc-4ed3-8a23-6ce0f48ab908",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Picking latest data from Source Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45ba8dca-dc64-4c9b-8f47-3030b1cecd65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "# yesterday’s file (assuming you process next day)\n",
    "#process_date = (datetime.today() - timedelta(days=1)).strftime(\"%Y/%m/%d\")\n",
    "process_date = datetime.today().strftime(\"%Y/%m/%d\") #--today's file\n",
    "\n",
    "path = f\"abfss://output@adlsdevvbsource001.dfs.core.windows.net/csv/{process_date}/*.parquet\"\n",
    "\n",
    "\n",
    "raw_df = spark.read.format(\"parquet\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(path)\n",
    "\n",
    "display(raw_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3ef3e2be-70fc-4277-acd4-a8f6b0a9244d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###DELTA TABLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c2e487e-ab56-4ba0-bd6b-bec47222db74",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "raw_df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").save(\"abfss://output@adlsdevvbsource001.dfs.core.windows.net/curated_csv/orderpay_scd2_delta\")\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS adbvb_7405617921830908.default.orderpay_scd2_delta\n",
    "USING DELTA\n",
    "LOCATION 'abfss://output@adlsdevvbsource001.dfs.core.windows.net/curated_csv/orderpay_scd2_delta'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1539b0b4-d8f1-40b8-afd0-533f435f4494",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###SCD2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d16f0a0-ce96-4652-9aba-b382becf59b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_date, current_timestamp, lit, expr\n",
    "from pyspark.sql.window import Window\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# =========================================\n",
    "# CONFIG\n",
    "# =========================================\n",
    "# Base folder where ADF writes CSV→Parquet bulk\n",
    "csv_base = \"abfss://output@adlsdevvbsource001.dfs.core.windows.net/csv\"\n",
    "\n",
    "# Delta target for SCD2 table\n",
    "delta_path = \"abfss://output@adlsdevvbsource001.dfs.core.windows.net/curated_csv/orderpay_scd2_delta\"\n",
    "\n",
    "# Business key for SCD2\n",
    "key_cols = [\"order_id\", \"payment_sequential\"]\n",
    "\n",
    "\n",
    "# =========================================\n",
    "# STEP 1: Find latest partition folder (YYYY/MM/DD)\n",
    "# =========================================\n",
    "years = dbutils.fs.ls(csv_base + \"/\")\n",
    "year_names = [f.name.rstrip('/') for f in years if f.isDir()]\n",
    "if not year_names:\n",
    "    raise ValueError(\"No year folders found under csv_base.\")\n",
    "\n",
    "latest_year = max(year_names)\n",
    "\n",
    "months = dbutils.fs.ls(f\"{csv_base}/{latest_year}/\")\n",
    "month_names = [f.name.rstrip('/') for f in months if f.isDir()]\n",
    "if not month_names:\n",
    "    raise ValueError(f\"No month folders found under year {latest_year}.\")\n",
    "\n",
    "latest_month = max(month_names)\n",
    "\n",
    "days = dbutils.fs.ls(f\"{csv_base}/{latest_year}/{latest_month}/\")\n",
    "day_names = [f.name.rstrip('/') for f in days if f.isDir()]\n",
    "if not day_names:\n",
    "    raise ValueError(f\"No day folders found under {latest_year}/{latest_month}.\")\n",
    "\n",
    "latest_day = max(day_names)\n",
    "\n",
    "latest_path = f\"{csv_base}/{latest_year}/{latest_month}/{latest_day}/*.parquet\"\n",
    "print(f\"Reading from: {latest_path}\")\n",
    "\n",
    "file_date = f\"{latest_year}-{latest_month}-{latest_day}\"\n",
    "\n",
    "\n",
    "# =========================================\n",
    "# STEP 2: Read raw data and add SCD2/audit columns\n",
    "# =========================================\n",
    "raw_df = spark.read.parquet(latest_path)\n",
    "\n",
    "new_df = (raw_df\n",
    "          .withColumn(\"ingest_date\", current_date())\n",
    "          .withColumn(\"file_date\", lit(file_date))\n",
    "          .withColumn(\"updated_at\", current_timestamp())\n",
    "          .withColumn(\"payment_dim_id\", expr(\"uuid()\"))\n",
    "          .withColumn(\"is_active\", lit(1))\n",
    "          .withColumn(\"start_date\", current_date())\n",
    "          .withColumn(\"end_date\", lit(None).cast(\"date\")))\n",
    "\n",
    "\n",
    "# =========================================\n",
    "# STEP 3: Initial load or align schema\n",
    "# =========================================\n",
    "if not DeltaTable.isDeltaTable(spark, delta_path):\n",
    "    print(\"Delta table path does not exist yet. Performing initial load.\")\n",
    "    (new_df.write\n",
    "        .format(\"delta\")\n",
    "        .mode(\"overwrite\")\n",
    "        .option(\"mergeSchema\", \"true\")\n",
    "        .save(delta_path))\n",
    "else:\n",
    "    print(\"Delta table exists. Aligning schema if needed, then performing SCD2 merge.\")\n",
    "    deltaTable = DeltaTable.forPath(spark, delta_path)\n",
    "    existing_columns = [f.name for f in deltaTable.toDF().schema.fields]\n",
    "\n",
    "    df = deltaTable.toDF()\n",
    "\n",
    "    # Ensure SCD2 columns\n",
    "    if \"payment_dim_id\" not in existing_columns:\n",
    "        df = df.withColumn(\"payment_dim_id\", lit(None).cast(\"string\"))\n",
    "    if \"is_active\" not in existing_columns:\n",
    "        df = df.withColumn(\"is_active\", lit(1))\n",
    "    if \"start_date\" not in existing_columns:\n",
    "        df = df.withColumn(\"start_date\", current_date())\n",
    "    if \"end_date\" not in existing_columns:\n",
    "        df = df.withColumn(\"end_date\", lit(None).cast(\"date\"))\n",
    "\n",
    "    # Ensure audit columns\n",
    "    if \"file_date\" not in existing_columns:\n",
    "        df = df.withColumn(\"file_date\", lit(None).cast(\"string\"))\n",
    "    if \"ingest_date\" not in existing_columns:\n",
    "        df = df.withColumn(\"ingest_date\", lit(None).cast(\"date\"))\n",
    "    if \"updated_at\" not in existing_columns:\n",
    "        df = df.withColumn(\"updated_at\", lit(None).cast(\"timestamp\"))\n",
    "\n",
    "    # Overwrite with aligned schema\n",
    "    (df.write\n",
    "        .format(\"delta\")\n",
    "        .mode(\"overwrite\")\n",
    "        .option(\"overwriteSchema\", \"true\")\n",
    "        .save(delta_path))\n",
    "\n",
    "    # Reload as DeltaTable for merge\n",
    "    deltaTable = DeltaTable.forPath(spark, delta_path)\n",
    "\n",
    "    # =========================================\n",
    "    # STEP 4: SCD2 merge\n",
    "    # =========================================\n",
    "    join_cond = \" AND \".join([f\"t.{c} = s.{c}\" for c in key_cols])\n",
    "\n",
    "    (deltaTable.alias(\"t\")\n",
    "        .merge(\n",
    "            new_df.alias(\"s\"),\n",
    "            join_cond\n",
    "        )\n",
    "        .whenMatchedUpdate(\n",
    "            condition=\"\"\"\n",
    "                t.is_active = 1 AND (\n",
    "                    t.payment_type <> s.payment_type OR\n",
    "                    t.payment_installments <> s.payment_installments OR\n",
    "                    t.payment_value <> s.payment_value\n",
    "                )\n",
    "            \"\"\",\n",
    "            set={\n",
    "                \"is_active\": \"0\",\n",
    "                \"end_date\": \"current_date()\"\n",
    "            }\n",
    "        )\n",
    "        .whenNotMatchedInsert(values={\n",
    "            \"payment_dim_id\": \"s.payment_dim_id\",\n",
    "            \"order_id\": \"s.order_id\",\n",
    "            \"payment_sequential\": \"s.payment_sequential\",\n",
    "            \"payment_type\": \"s.payment_type\",\n",
    "            \"payment_installments\": \"s.payment_installments\",\n",
    "            \"payment_value\": \"s.payment_value\",\n",
    "            \"ingest_date\": \"s.ingest_date\",\n",
    "            \"file_date\": \"s.file_date\",\n",
    "            \"updated_at\": \"s.updated_at\",\n",
    "            \"is_active\": \"1\",\n",
    "            \"start_date\": \"current_date()\",\n",
    "            \"end_date\": \"null\"\n",
    "        })\n",
    "        .execute())\n",
    "\n",
    "\n",
    "# =========================================\n",
    "# STEP 5: Check results\n",
    "# =========================================\n",
    "print(\"Before Merge (raw_df):\")\n",
    "raw_df.show(10, truncate=False)\n",
    "\n",
    "print(\"After Merge (Delta SCD2 table):\")\n",
    "updated_df = spark.read.format(\"delta\").load(delta_path)\n",
    "updated_df.show(20, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4550d323-665a-4ec8-9ace-dbf3ea6fad8c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7906958246330523,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "SCD2_csv",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
