{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "339c11d9-5276-498d-b7be-f9fed47dfb17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "configs = {\n",
    "  \"fs.azure.account.auth.type\": \"OAuth\",\n",
    "  \"fs.azure.account.oauth.provider.type\": \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n",
    "  \"fs.azure.account.oauth2.client.id\": \"3e04ee23-1ccd-49ad-8945-2ebce8253fc2\",\n",
    "  \"fs.azure.account.oauth2.client.secret\": dbutils.secrets.get(\"scope\", \"key\"),\n",
    "  \"fs.azure.account.oauth2.client.endpoint\": \"https://login.microsoftonline.com/9b91be9b-3f50-4b53-ba3b-d896a492064e/oauth2/token\"\n",
    "}\n",
    "dbutils.fs.mount(\n",
    "  source = \"abfss://output@adlsdevvbsource001.dfs.core.windows.net/\",\n",
    "  mount_point = \"/mnt/source\",\n",
    "  extra_configs = configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ad39776-f9f1-4d58-a361-f8ac508ac53e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%fs\n",
    "ls '/mnt/source/snowflake'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd80dc88-af83-4206-8eac-b9977bde2b58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "# yesterdayâ€™s file (assuming you process next day)\n",
    "#process_date = (datetime.today() - timedelta(days=1)).strftime(\"%Y/%m/%d\")\n",
    "process_date = datetime.today().strftime(\"%Y/%m/%d\") #--today's file\n",
    "\n",
    "path = f\"/mnt/source/snowflake/{process_date}/*.parquet\"\n",
    "\n",
    "\n",
    "raw_df = spark.read.format(\"parquet\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(path)\n",
    "\n",
    "display(raw_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbb9e978-b9ac-41dd-99de-8ced9128236e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "raw_df.write.format(\"delta\").mode(\"overwrite\").save(\"/mnt/curated/cust_order_delta\")\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS cust_order_delta USING DELTA LOCATION '/mnt/curated/cust_order_delta'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a113c060-53b6-485b-8a5a-9ab76b8c2e2e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###SCD1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1bebf56c-79c0-44f7-93fe-90e697e4b492",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_date, lit, row_number\n",
    "from pyspark.sql.window import Window\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# ---- Step 1: Get latest file path and folder date ----\n",
    "folders = dbutils.fs.ls(\"/mnt/source/snowflake/\")\n",
    "latest_year = max([f.name.replace('/', '') for f in folders])\n",
    "\n",
    "folders = dbutils.fs.ls(f\"/mnt/source/snowflake/{latest_year}/\")\n",
    "latest_month = max([f.name.replace('/', '') for f in folders])\n",
    "\n",
    "folders = dbutils.fs.ls(f\"/mnt/source/snowflake/{latest_year}/{latest_month}/\")\n",
    "latest_day = max([f.name.replace('/', '') for f in folders])\n",
    "\n",
    "latest_path = f\"/mnt/source/snowflake/{latest_year}/{latest_month}/{latest_day}/*.parquet\"\n",
    "print(f\"Reading from: {latest_path}\")\n",
    "\n",
    "# Construct file_date from folder\n",
    "file_date = f\"{latest_year}-{latest_month}-{latest_day}\"\n",
    "\n",
    "# ---- Step 2: Read raw data and add columns ----\n",
    "raw_df = spark.read.parquet(latest_path)\n",
    "\n",
    "spark.conf.set(\"spark.databricks.delta.schema.autoMerge.enabled\", \"true\")\n",
    "\n",
    "new_df = (raw_df\n",
    "          .withColumn(\"ingest_date\", current_date())   # actual load date\n",
    "          .withColumn(\"file_date\", lit(file_date)))    # folder date\n",
    "\n",
    "# ---- Step 2.1: Deduplicate (avoid merge errors) ----\n",
    "window = Window.partitionBy(\"ORDER_ID\", \"ORDER_ITEM_ID\").orderBy(new_df[\"ingest_date\"].desc())\n",
    "new_df = (new_df\n",
    "          .withColumn(\"rn\", row_number().over(window))\n",
    "          .filter(\"rn = 1\")   # keep only latest record per ORDER_ID + ORDER_ITEM_ID\n",
    "          .drop(\"rn\"))\n",
    "\n",
    "# ---- Step 3: Merge into Delta (SCD1) ----\n",
    "delta_path = \"/mnt/curated/cust_order_delta\"\n",
    "\n",
    "if not DeltaTable.isDeltaTable(spark, delta_path):\n",
    "    # Initial load\n",
    "    (new_df.write\n",
    "        .format(\"delta\")\n",
    "        .mode(\"overwrite\")\n",
    "        .option(\"mergeSchema\", \"true\")\n",
    "        .save(delta_path))\n",
    "else:\n",
    "    # Merge (SCD1: overwrite on match, insert on new)\n",
    "    deltaTable = DeltaTable.forPath(spark, delta_path)\n",
    "    (deltaTable.alias(\"t\")\n",
    "     .merge(new_df.alias(\"s\"),\n",
    "            \"t.ORDER_ID = s.ORDER_ID AND t.ORDER_ITEM_ID = s.ORDER_ITEM_ID\")\n",
    "     .whenMatchedUpdateAll()\n",
    "     .whenNotMatchedInsertAll()\n",
    "     .execute())\n",
    "\n",
    "# ---- Step 4: Reload and check ----\n",
    "updated_df = (spark.read\n",
    "              .format(\"delta\")\n",
    "              .option(\"mergeSchema\", \"true\")\n",
    "              .load(delta_path))\n",
    "\n",
    "print(\"Before Merge:\")\n",
    "raw_df.show(10, truncate=False)\n",
    "\n",
    "print(\"After Merge:\")\n",
    "updated_df.show(10, truncate=False)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6810459256770177,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "SCD1",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
