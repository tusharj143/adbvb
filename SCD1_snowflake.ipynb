{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd80dc88-af83-4206-8eac-b9977bde2b58",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1767693022730}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "# yesterdayâ€™s file (assuming you process next day)\n",
    "#process_date = (datetime.today() - timedelta(days=1)).strftime(\"%Y/%m/%d\")\n",
    "process_date = datetime.today().strftime(\"%Y/%m/%d\") #--today's file\n",
    "\n",
    "path = f\"abfss://output@adlsdevvbsource001.dfs.core.windows.net/snow/{process_date}/*.parquet\"\n",
    "\n",
    "\n",
    "raw_df = spark.read.format(\"parquet\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(path)\n",
    "\n",
    "display(raw_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbb9e978-b9ac-41dd-99de-8ced9128236e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "target_path = \"abfss://output@adlsdevvbsource001.dfs.core.windows.net/curated_snowflake/order_items_delta\"\n",
    "\n",
    "(raw_df.write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "    .save(target_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c88e76b1-e2bf-4603-bddb-b7817c016bc8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE TABLE IF NOT EXISTS adbvb_7405617921830908.default.order_items_delta\n",
    "USING DELTA\n",
    "LOCATION \"abfss://output@adlsdevvbsource001.dfs.core.windows.net/curated_snowflake/order_items_delta\";\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d61be41-5ca2-41c2-8cca-b381f624e746",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1767691744611}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SHOW CATALOGS;\n",
    "SHOW SCHEMAS IN adbvb_7405617921830908;\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95c5eab8-7259-404d-a85e-cfceee5c4093",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.table(\"adbvb_7405617921830908.default.order_items_delta\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a113c060-53b6-485b-8a5a-9ab76b8c2e2e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###SCD1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1bebf56c-79c0-44f7-93fe-90e697e4b492",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pyspark.sql.functions import current_date, lit, row_number\n",
    "from pyspark.sql.window import Window\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# ==============================\n",
    "# CONFIG\n",
    "# ==============================\n",
    "# Base folder where Snowflake parquet exports land\n",
    "snowflake_base = \"abfss://output@adlsdevvbsource001.dfs.core.windows.net/snow\"\n",
    "\n",
    "# Delta target folder on ADLS for SCD1 table\n",
    "delta_path = \"abfss://output@adlsdevvbsource001.dfs.core.windows.net/curated_snowflake/order_items_delta\"\n",
    "\n",
    "# Columns that define the business key (SCD1 merge keys)\n",
    "key_cols = [\"ORDER_ID\", \"ORDER_ITEM_ID\"]    # change if your keys are different\n",
    "\n",
    "# ==============================\n",
    "# STEP 1: Find latest partition folder (YYYY/MM/DD)\n",
    "# ==============================\n",
    "\n",
    "# List years\n",
    "years = dbutils.fs.ls(snowflake_base + \"/\")\n",
    "year_names = [f.name.rstrip('/') for f in years if f.isDir()]\n",
    "if not year_names:\n",
    "    raise ValueError(\"No year folders found under snowflake_base.\")\n",
    "\n",
    "latest_year = max(year_names)\n",
    "\n",
    "# List months for that year\n",
    "months = dbutils.fs.ls(f\"{snowflake_base}/{latest_year}/\")\n",
    "month_names = [f.name.rstrip('/') for f in months if f.isDir()]\n",
    "if not month_names:\n",
    "    raise ValueError(f\"No month folders found under year {latest_year}.\")\n",
    "\n",
    "latest_month = max(month_names)\n",
    "\n",
    "# List days for that year/month\n",
    "days = dbutils.fs.ls(f\"{snowflake_base}/{latest_year}/{latest_month}/\")\n",
    "day_names = [f.name.rstrip('/') for f in days if f.isDir()]\n",
    "if not day_names:\n",
    "    raise ValueError(f\"No day folders found under {latest_year}/{latest_month}.\")\n",
    "\n",
    "latest_day = max(day_names)\n",
    "\n",
    "latest_path = f\"{snowflake_base}/{latest_year}/{latest_month}/{latest_day}/*.parquet\"\n",
    "print(f\"Reading from: {latest_path}\")\n",
    "\n",
    "file_date = f\"{latest_year}-{latest_month}-{latest_day}\"\n",
    "\n",
    "# ==============================\n",
    "# STEP 2: Read raw data and add columns\n",
    "# ==============================\n",
    "raw_df = spark.read.parquet(latest_path)\n",
    "\n",
    "spark.conf.set(\"spark.databricks.delta.schema.autoMerge.enabled\", \"true\")\n",
    "\n",
    "new_df = (raw_df\n",
    "          .withColumn(\"ingest_date\", current_date())   # load date\n",
    "          .withColumn(\"file_date\", lit(file_date)))    # folder date\n",
    "\n",
    "# ==============================\n",
    "# STEP 2.1: Deduplicate on key columns\n",
    "# ==============================\n",
    "window = Window.partitionBy(*key_cols).orderBy(new_df[\"ingest_date\"].desc())\n",
    "\n",
    "new_df = (new_df\n",
    "          .withColumn(\"rn\", row_number().over(window))\n",
    "          .filter(\"rn = 1\")\n",
    "          .drop(\"rn\"))\n",
    "\n",
    "# ==============================\n",
    "# STEP 3: Merge into Delta (SCD1)\n",
    "# ==============================\n",
    "if not DeltaTable.isDeltaTable(spark, delta_path):\n",
    "    print(\"Delta table path does not exist yet. Performing initial load.\")\n",
    "    (new_df.write\n",
    "        .format(\"delta\")\n",
    "        .mode(\"overwrite\")\n",
    "        .option(\"mergeSchema\", \"true\")\n",
    "        .save(delta_path))\n",
    "else:\n",
    "    print(\"Delta table exists. Performing SCD1 merge.\")\n",
    "    deltaTable = DeltaTable.forPath(spark, delta_path)\n",
    "\n",
    "    # Build join condition string from key_cols\n",
    "    join_cond = \" AND \".join([f\"t.{c} = s.{c}\" for c in key_cols])\n",
    "\n",
    "    (deltaTable.alias(\"t\")\n",
    "        .merge(new_df.alias(\"s\"), join_cond)\n",
    "        .whenMatchedUpdateAll()\n",
    "        .whenNotMatchedInsertAll()\n",
    "        .execute())\n",
    "\n",
    "# ==============================\n",
    "# STEP 4: Reload and quick check\n",
    "# ==============================\n",
    "updated_df = (spark.read\n",
    "              .format(\"delta\")\n",
    "              .option(\"mergeSchema\", \"true\")\n",
    "              .load(delta_path))\n",
    "\n",
    "print(\"Sample from source (raw_df):\")\n",
    "raw_df.show(10, truncate=False)\n",
    "\n",
    "print(\"Sample from SCD1 Delta (updated_df):\")\n",
    "updated_df.show(10, truncate=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5316195292761297,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "SCD1_snowflake",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
